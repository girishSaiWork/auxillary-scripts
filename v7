from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, current_timestamp, split, element_at, 
    to_timestamp, size, array_contains, when, lit, length, 
    concat_ws, array_union, array, struct
)
from pyspark.sql.types import StructType
import json
from collections import defaultdict

# Initialize Spark
# spark = SparkSession.builder.appName("SilverProcessing_V5").enableHiveSupport().getOrCreate()

# -------------------------------------------------------------------------
# LOAD METADATA HELPERS
# -------------------------------------------------------------------------
def get_active_schema(dataset_name):
    """Fetches the active schema JSON from meta_schema_registry"""
    row = spark.table("meta_schema_registry") \
        .filter((col("dataset_name") == dataset_name) & (col("is_active") == True)) \
        .select("schema_json") \
        .head()
    if row:
        return StructType.fromJson(json.loads(row.schema_json))
    else:
        raise Exception(f"No active schema found for {dataset_name}")

def get_dq_policies(dataset_name):
    """Fetches DQ rules as a list of dictionaries"""
    return spark.table("meta_dq_policy") \
        .filter(col("dataset_name") == dataset_name) \
        .collect()

# -------------------------------------------------------------------------
# CONFIGURATION
# -------------------------------------------------------------------------
CONFIG_PATH = "e:/Study Space/Analytics Enginerring/Data Engineering/Azure Databricks/Kafka/config.json"
with open(CONFIG_PATH, 'r') as f:
    config = json.load(f)

base_path = config["storage"]["basePath"]
DB_NAME = config["database"]["name"]
TRIGGER_MODE = config.get("trigger", {}).get("type", "availableNow")

BRONZE_TABLE = f"{DB_NAME}.{config['rawZoneDeltaTableInformation']['tableName']}"
DATASET_NAME = "event_log"
SILVER_TABLE_NAME = config['curatedDeltaTableInformation']['tableName']
SILVER_ERROR_TABLE_NAME = f"{SILVER_TABLE_NAME}_error"
SILVER_OUTPUT_PATH = f"{base_path}{config['curatedDeltaTableInformation']['path']}"
SILVER_ERROR_PATH = f"{base_path}{config['curatedDeltaTableInformation']['errorPath']}"
CHECKPOINT_PATH = f"{base_path}{config['curatedDeltaTableInformation']['checkpoint']}"

# -------------------------------------------------------------------------
# HELPER: Error Appender
# -------------------------------------------------------------------------
def _add_error(df, condition, rule_id, col_name, msg, include_scope, exclude_scope):
    """
    Internal helper to handle the common logic of Scope Checking + Error Appending.
    df: DataFrame
    condition: Boolean Column Expression (True=Pass, False=Fail)
    """
    # 1. Determine Scope
    app_code = col("applicationCode")
    scope_ok = (lit("ALL").isin(include_scope) | app_code.isin(include_scope)) & \
               (~app_code.isin(exclude_scope))
    
    # 2. Determine Failure (In Scope AND Check Failed)
    is_error = scope_ok & (~condition)
    
    # 3. Append to Error Array
    error_struct = struct(
        lit(rule_id).alias("rule_id"), 
        lit(col_name).alias("column"), 
        lit(msg).alias("message")
    )

    # Note: We update "dq_errors" and "is_valid"
    return df.withColumn("dq_errors", 
        when(is_error, array_union(col("dq_errors"), array(error_struct)))
        .otherwise(col("dq_errors"))
    ).withColumn("is_valid", col("is_valid") & (~is_error))

# -------------------------------------------------------------------------
# DQ FUNCTION DEFINITIONS (The "V5" Request)
# -------------------------------------------------------------------------
# Each function validates a list of rules for its specific type.

def dq_not_null(df, rules):
    """
    Applies all 'Not Null' checks.
    """
    for r in rules:
        col_name = r["target_column"]
        if col_name in df.columns:
            # Logic: isNotNull()
            cond = col(col_name).isNotNull()
            # Apply
            df = _add_error(df, cond, r["policy_id"], col_name, r["error_message"], 
                           r["app_scope_inclusion"], r["app_scope_exclusion"])
    return df

def dq_regex_pattern(df, rules):
    """
    Applies all 'Regex' checks.
    """
    for r in rules:
        col_name = r["target_column"]
        pattern = r["rule_params"].get("pattern")
        if col_name in df.columns:
            cond = col(col_name).rlike(pattern)
            df = _add_error(df, cond, r["policy_id"], col_name, r["error_message"], 
                           r["app_scope_inclusion"], r["app_scope_exclusion"])
    return df

def dq_min_length(df, rules):
    """
    Applies all 'Min Length' checks.
    """
    for r in rules:
        col_name = r["target_column"]
        length_val = int(r["rule_params"].get("length"))
        if col_name in df.columns:
            cond = length(col(col_name)) >= length_val
            df = _add_error(df, cond, r["policy_id"], col_name, r["error_message"], 
                           r["app_scope_inclusion"], r["app_scope_exclusion"])
    return df

def dq_is_iso_timestamp(df, rules):
    """
    Applies all 'ISO Timestamp' checks.
    """
    for r in rules:
        col_name = r["target_column"]
        if col_name in df.columns:
            # Check: Parse ISO8601
            cond = to_timestamp(col(col_name), "yyyy-MM-dd'T'HH:mm:ss.SSSXXX").isNotNull()
            df = _add_error(df, cond, r["policy_id"], col_name, r["error_message"], 
                           r["app_scope_inclusion"], r["app_scope_exclusion"])
    return df

def dq_array_not_empty(df, rules):
    """
    Applies all 'Array Not Empty' checks.
    """
    for r in rules:
        col_name = r["target_column"]
        if col_name in df.columns:
            cond = size(col(col_name)) > 0
            df = _add_error(df, cond, r["policy_id"], col_name, r["error_message"], 
                           r["app_scope_inclusion"], r["app_scope_exclusion"])
    return df

# -------------------------------------------------------------------------
# ORCHESTRATOR
# -------------------------------------------------------------------------
def apply_dynamic_dq(df, policies):
    # 1. Initialize Columns
    df = df.withColumn("dq_errors", array().cast("array<struct<rule_id:string, column:string, message:string>>")) \
           .withColumn("is_valid", lit(True))
           
    # 2. Group Policies by Type
    grouped_rules = defaultdict(list)
    for p in policies:
        grouped_rules[p["rule_type"]].append(p)
        
    # 3. Call Specific Functions
    # This structure is extremely explicit and debuggable.
    
    if "not_null" in grouped_rules:
        df = dq_not_null(df, grouped_rules["not_null"])
        
    if "min_length" in grouped_rules:
        df = dq_min_length(df, grouped_rules["min_length"])
        
    if "regex_pattern" in grouped_rules:
        df = dq_regex_pattern(df, grouped_rules["regex_pattern"])
        
    if "is_iso_timestamp" in grouped_rules:
        df = dq_is_iso_timestamp(df, grouped_rules["is_iso_timestamp"])
        
    if "array_not_empty" in grouped_rules:
        df = dq_array_not_empty(df, grouped_rules["array_not_empty"])
        
    # Note: If a new rule type appears in metadata but not here, it is skipped (Safe).
    
    return df

def process_silver():
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {DB_NAME}")

    print(f"Reading from: {BRONZE_TABLE}")
    bronze_df = (spark.readStream
                 .option("ignoreChanges", "true") 
                 .table(BRONZE_TABLE)) 
                 
    transformed_df = bronze_df

    print("Fetching Policies...")
    policies = get_dq_policies(DATASET_NAME)
    
    print("Applying DQ Rules (V5 Strategy)...")
    dq_df = apply_dynamic_dq(transformed_df, policies)

    trigger_opts = {"availableNow": True} if TRIGGER_MODE == "availableNow" else {"processingTime": "5 seconds"}

    def write_microbatch(batch_df, batch_id):
        batch_df.persist()
        
        valid_records = batch_df.filter(col("is_valid") == True).drop("is_valid", "dq_errors")
        error_records = batch_df.filter(col("is_valid") == False)
        
        print(f"Batch {batch_id}: Valid={valid_records.count()}, Error={error_records.count()}")

        if valid_records.count() > 0:
            (valid_records.write
             .format("delta")
             .mode("append")
             .partitionBy("eventDate")
             .option("mergeSchema", "true")
             .option("path", SILVER_OUTPUT_PATH) 
             .saveAsTable(f"{DB_NAME}.{SILVER_TABLE_NAME}")) 
             
        if error_records.count() > 0:
            (error_records.write
             .format("delta")
             .mode("append")
             .partitionBy("eventDate")
             .option("mergeSchema", "true")
             .option("path", SILVER_ERROR_PATH) 
             .saveAsTable(f"{DB_NAME}.{SILVER_ERROR_TABLE_NAME}"))
             
        batch_df.unpersist()

    query = (dq_df.writeStream
             .foreachBatch(write_microbatch)
             .trigger(**trigger_opts)
             .option("checkpointLocation", CHECKPOINT_PATH)
             .start())
    
    print(f"Silver Stream V5 started...")
    query.awaitTermination()

if __name__ == "__main__":
    process_silver()
